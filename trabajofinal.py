# -*- coding: utf-8 -*-
"""Ultima version Ultima.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jksCNFvqaH9JEgY6_P27hjuB_rlaDvHn

Primero debemos cargar las imágenes y llevarlas a la misma transformación geométrica y sistema de coordenaas de referencia.
Para ello se requiere la biblioteca gdal.
"""



"""Tenemos tres imagenes por lo tanto vamos a trabajar con tres dataset."""


import tifffile
import os
import numpy as np
import matplotlib.pyplot as plt
import numpy as np
from skimage.transform import resize
from sklearn.preprocessing import MinMaxScaler
import torch
from skimage.util import view_as_windows
from skimage import transform
from torch.utils.data import Dataset
import torch.nn as nn
import asyncio
import threading
import time
import multiprocessing
#%%
# Ruta de la carpeta que contiene las imágenes TIFF
ruta_carpeta = r'D:\Cursos\Computacion Paralela\Trabajo de computacion paralela\20210226'

# Inicializar una lista para almacenar las imágenes TIFF
bandas_landsat = []

# Iterar sobre los archivos en la carpeta
for archivo in os.listdir(ruta_carpeta):
    if archivo.endswith(".tif"):  # Verificar que el archivo sea TIFF
        ruta_completa = os.path.join(ruta_carpeta, archivo)
        imagen = tifffile.imread(ruta_completa)
        bandas_landsat.append(imagen)

# Convierte la lista de imágenes TIFF en un arreglo numpy
bandas_landsat = np.stack(bandas_landsat, axis=-1)

# Verifica las dimensiones del arreglo de imágenes TIFF
print(bandas_landsat.shape)  # Salida: (altura, anchura, num_imagenes)

#%%

# Obtener el valor máximo y mínimo de cada banda Landsat
maximos = np.max(bandas_landsat, axis=(0, 1))
minimos = np.min(bandas_landsat, axis=(0, 1))

# Mostrar los valores máximo y mínimo de cada banda
for i, (maximo, minimo) in enumerate(zip(maximos, minimos)):
    print(f"Banda {i+1}: Valor máximo = {maximo}, Valor mínimo = {minimo}")

# Visualizar la primera imagen
plt.imshow(bandas_landsat[..., 0], cmap='jet')
plt.title('Banda 1')
plt.show()
#%%

# bandas_landsat contiene las imágenes de las bandas de Landsat en un arreglo numpy

# Verificar si hay valores NaN en las imágenes de las bandas de Landsat
hay_nan = np.isnan(bandas_landsat)

# Contar la cantidad de valores NaN en cada banda
cantidad_nan_por_banda = np.sum(hay_nan, axis=(0, 1))

# Verificar si hay valores NaN en alguna banda
hay_nan_en_alguna_banda = np.any(cantidad_nan_por_banda > 0)

# Imprimir los resultados
print("¿Hay valores NaN en alguna banda?:", hay_nan_en_alguna_banda)
print("Cantidad de NaN por banda:", cantidad_nan_por_banda)


#%%
# Ruta de la carpeta que contiene los mapas de humedad
ruta_mapa =  r'D:\Cursos\Computacion Paralela\Trabajo de computacion paralela\mapa humedad'

# Inicializar una lista para almacenar las imágenes TIFF
mapa_humedad = []

# Iterar sobre los archivos en la carpeta
for archivom in os.listdir(ruta_mapa):
    if archivom.endswith(".tif"):  # Verificar que el archivo sea TIFF
        ruta_completam = os.path.join(ruta_mapa, archivom)
        imagenm = tifffile.imread(ruta_completam)
        mapa_humedad.append(imagenm)

# Convierte la lista de imágenes TIFF en un arreglo numpy
mapa_humedad = np.stack(mapa_humedad, axis=-1)

# Verifica las dimensiones del arreglo de imágenes TIFF
print(mapa_humedad.shape)
#%%

valor_minimo = np.min(mapa_humedad)
valor_maximo = np.max(mapa_humedad)

print("Valor mínimo:", valor_minimo)
print("Valor máximo:", valor_maximo)
# Visualizar la imagen del mapa de humedad
plt.imshow(mapa_humedad, cmap='jet')
plt.title('Mapa de Humedad')
plt.colorbar(label='Humedad')
plt.show()

# Reemplazar los valores nan por 0
mapa_humedad_resized = np.nan_to_num(mapa_humedad, nan=0)
mapa_humedad_resized[mapa_humedad_resized < 0] = 0

# Visualizar el mapa de humedad
plt.imshow(mapa_humedad_resized, cmap='jet')
plt.title('Mapa de Humedad')
plt.colorbar(label='Humedad')
plt.show()
mapa_humedad=mapa_humedad_resized
#%%

# Redimensionar el mapa de humedad utilizando la función resize de scikit-image
mapa_humedad_resized = resize(mapa_humedad, (1494, 799))

# Verificar las dimensiones del mapa de humedad redimensionado
print(mapa_humedad_resized.shape)

import matplotlib.pyplot as plt
valor_minimo = np.min(mapa_humedad_resized)
valor_maximo = np.max(mapa_humedad_resized)

print("Valor mínimo:", valor_minimo)
print("Valor máximo:", valor_maximo)
# Graficar la imagen resultante
plt.imshow(mapa_humedad_resized, cmap='jet')
plt.colorbar()
plt.title('Mapa de Humedad Redimensionado')
plt.show()
#%%

# Supongamos que 'mapa_humedad' es tu matriz de humedad

# Crear una instancia del objeto MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))

# Reshape de la matriz para que tenga una dimensión adecuada para el escalado
mapa_humedad_reshaped = np.reshape(mapa_humedad, (-1, 1))

# Normalizar los valores de la matriz en una escala de 0 a 1
mapa_humedad_normalizado = scaler.fit_transform(mapa_humedad_reshaped)

# Reshape nuevamente a la forma original
mapa_humedad_normalizado = np.reshape(mapa_humedad_normalizado, mapa_humedad.shape)

# Visualizar el mapa de humedad normalizado
plt.imshow(mapa_humedad_normalizado, cmap='jet')
plt.title('Mapa de Humedad Normalizado')
plt.colorbar(label='Humedad (Normalizado)')
plt.show()

mapa_humedad=mapa_humedad_normalizado
#%%
# Dimensiones de la imagen de Landsat y el mapa de humedad
landsat_shape = (1494, 799)  # Dimensiones de la imagen de Landsat
humedad_shape = (1494, 799)  # Dimensiones del mapa de humedad

# Calcular las coordenadas aproximadas
landsat_height = landsat_shape[0]
landsat_width = landsat_shape[1]

# Coordenadas para entrenamiento (60% inferior)
train_coords = [(int(landsat_height * 0.4), 0), (landsat_height, landsat_width)]

# Coordenadas para validación (20% superior derecho)
val_coords = [(0, int(landsat_width * 0.8)), (int(landsat_height * 0.4), landsat_width)]

# Coordenadas para prueba (20% superior izquierdo)
test_coords = [(0, 0), (int(landsat_height * 0.4), int(landsat_width * 0.8))]

# Imprimir las coordenadas rectificadas
print("Train coordinates: ", train_coords)
print("Validation coordinates: ", val_coords)
print("Test coordinates: ", test_coords)

print(bandas_landsat.shape)

# Dividir las imágenes de Landsat en conjuntos de entrenamiento, validación y prueba
train_landsat = bandas_landsat[train_coords[0][0]:train_coords[1][0], train_coords[0][1]:train_coords[1][1], :]
val_landsat = bandas_landsat[val_coords[0][0]:val_coords[1][0], val_coords[0][1]:val_coords[1][1], :]
test_landsat = bandas_landsat[test_coords[0][0]:test_coords[1][0], test_coords[0][1]:test_coords[1][1], :]

# Dividir el mapa de humedad en conjuntos de entrenamiento, validación y prueba
train_humedad = mapa_humedad_resized[train_coords[0][0]:train_coords[1][0], train_coords[0][1]:train_coords[1][1]]
val_humedad = mapa_humedad_resized[val_coords[0][0]:val_coords[1][0], val_coords[0][1]:val_coords[1][1]]
test_humedad = mapa_humedad_resized[test_coords[0][0]:test_coords[1][0], test_coords[0][1]:test_coords[1][1]]

# Verificar las dimensiones de los conjuntos divididos
print("Dimensiones del conjunto de entrenamiento de Landsat:", train_landsat.shape)
print("Dimensiones del conjunto de validación de Landsat:", val_landsat.shape)
print("Dimensiones del conjunto de prueba de Landsat:", test_landsat.shape)
print("Dimensiones del conjunto de entrenamiento de humedad:", train_humedad.shape)
print("Dimensiones del conjunto de validación de humedad:", val_humedad.shape)
print("Dimensiones del conjunto de prueba de humedad:", test_humedad.shape)
#%%
# Tamaño de los parches
patch_size = (64, 64, bandas_landsat.shape[2])  # Tamaño del parche considerando los canales de Landsat

# Dividir el conjunto de entrenamiento en parches
train_patches = view_as_windows(train_landsat, patch_size, step=patch_size)
train_patches = train_patches.reshape(-1, *patch_size)

# Dividir el conjunto de validación en parches
val_patches = view_as_windows(val_landsat, patch_size, step=patch_size)
val_patches = val_patches.reshape(-1, *patch_size)

# Dividir el conjunto de prueba en parches
test_patches = view_as_windows(test_landsat, patch_size, step=patch_size)
test_patches = test_patches.reshape(-1, *patch_size)

# Dividir el mapa de humedad en parches correspondientes
patch_size_humidity = (64, 64, 1)  # Tamaño del parche considerando un solo canal de humedad

train_humidity_patches = view_as_windows(train_humedad, patch_size_humidity, step=patch_size_humidity)
train_humidity_patches = train_humidity_patches.reshape(-1, *patch_size_humidity)

val_humidity_patches = view_as_windows(val_humedad, patch_size_humidity, step=patch_size_humidity)
val_humidity_patches = val_humidity_patches.reshape(-1, *patch_size_humidity)

test_humidity_patches = view_as_windows(test_humedad, patch_size_humidity, step=patch_size_humidity)
test_humidity_patches = test_humidity_patches.reshape(-1, *patch_size_humidity)

# Verificar las dimensiones de los conjuntos de parches
print("Dimensiones del conjunto de parches de entrenamiento de Landsat:", train_patches.shape)
print("Dimensiones del conjunto de parches de validación de Landsat:", val_patches.shape)
print("Dimensiones del conjunto de parches de prueba de Landsat:", test_patches.shape)
print("Dimensiones del conjunto de parches de entrenamiento de humedad:", train_humidity_patches.shape)
print("Dimensiones del conjunto de parches de validación de humedad:", val_humidity_patches.shape)
print("Dimensiones del conjunto de parches de prueba de humedad:", test_humidity_patches.shape)
#%%
# Seleccionar un canal específico para visualizar
canal = 0  # Puedes cambiar este valor para seleccionar un canal diferente

# Visualizar el primer parche de entrenamiento
plt.imshow(train_patches[0, :, :, canal], cmap='jet')
plt.title('Primer parche de entrenamiento')
plt.show()

# Visualizar el primer parche de validación
plt.imshow(val_patches[0, :, :, canal], cmap='jet')
plt.title('Primer parche de validación')
plt.show()

# Visualizar el primer parche de prueba
plt.imshow(test_patches[0, :, :, canal], cmap='jet')
plt.title('Primer parche de prueba')
plt.show()


#%%
from skimage.transform import resize

def agregar_ruido(parche, media=0, desviacion=0.05):
    # Generar ruido gaussiano con la misma forma que el parche
    ruido = np.random.normal(loc=media, scale=desviacion, size=parche.shape)

    # Sumar el ruido al parche
    parche_ruidoso = parche + ruido

    # Asegurarse de que los valores estén en el rango [0, 1]
    parche_ruidoso = np.clip(parche_ruidoso, 0, 1)

    return parche_ruidoso


def aplicar_aumento_datos(parches_landsat, parches_humedad, factor_aumento=2):
    parches_landsat_aumentados = []
    parches_humedad_aumentados = []

    for i in range(len(parches_landsat)):
        parche_landsat = parches_landsat[i]
        parche_humedad = parches_humedad[i]

        parches_landsat_aumentados.append(parche_landsat)
        parches_humedad_aumentados.append(parche_humedad)

        for _ in range(factor_aumento):


            # Agregar ruido a los parches transformados
            parche_landsat_ruidoso = agregar_ruido(parche_landsat)
            parche_humedad_ruidoso = agregar_ruido(parche_humedad)


            parches_landsat_aumentados.append(parche_landsat_ruidoso)


            parches_humedad_aumentados.append(parche_humedad_ruidoso)

    return np.array(parches_landsat_aumentados), np.array(parches_humedad_aumentados)

# Aplicar aumento de datos a los conjuntos de parches de Landsat y humedad
train_patches_landsat_aumentados, train_patches_humedad_aumentados = aplicar_aumento_datos(train_patches, train_humidity_patches)
val_patches_landsat_aumentados, val_patches_humedad_aumentados = aplicar_aumento_datos(val_patches, val_humidity_patches)

#%%
from torch.utils.data import Dataset

class PatchDataset(torch.utils.data.Dataset):
    def __init__(self, patches, humidity_patches):
        self.patches = patches
        self.humidity_patches = humidity_patches

    def __len__(self):
        return len(self.patches)

    def __getitem__(self, idx):
        patch = self.patches[idx]
        humidity_patch = self.humidity_patches[idx]
        return patch, humidity_patch


def create_dataset(train_patches, val_patches, train_humidity_patches, val_humidity_patches):
    train_dataset = PatchDataset(train_patches, train_humidity_patches)
    val_dataset = PatchDataset(val_patches, val_humidity_patches)
    
    return train_dataset, val_dataset

#%%
import torch.nn as nn

# Definir la clase DoubleConv con capa de normalización y función de activación ReLU
class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(DoubleConv, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.conv(x)

# Definir la arquitectura de U-Net mejorada
class UNet(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(UNet, self).__init__()
        self.encoder1 = DoubleConv(in_channels, 64)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.encoder2 = DoubleConv(64, 128)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.encoder3 = DoubleConv(128, 256)
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.encoder4 = DoubleConv(256, 512)
        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.bottleneck = DoubleConv(512, 1024)

        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)
        self.decoder1 = DoubleConv(1024, 512)
        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)
        self.decoder2 = DoubleConv(512, 256)
        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
        self.decoder3 = DoubleConv(256, 128)
        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.decoder4 = DoubleConv(128, 64)

        self.conv_out = nn.Conv2d(64, out_channels, kernel_size=1)
        self.sigmoid = nn.Sigmoid()
        self.norm = nn.BatchNorm2d(out_channels)

        self.dropout = nn.Dropout2d(p=0.2)  # Regularización de abandono (dropout)
        self.l2_regularization = nn.MSELoss()

    def forward(self, x):
        enc1 = self.encoder1(x)
        pool1 = self.pool1(enc1)
        enc2 = self.encoder2(pool1)
        pool2 = self.pool2(enc2)
        enc3 = self.encoder3(pool2)
        pool3 = self.pool3(enc3)
        enc4 = self.encoder4(pool3)
        pool4 = self.pool4(enc4)

        bottleneck = self.bottleneck(pool4)

        up1 = self.upconv1(bottleneck)
        up1 = torch.cat([up1, enc4], dim=1)
        dec1 = self.decoder1(up1)
        dec1 = self.dropout(dec1)  # Aplicar dropout

        up2 = self.upconv2(dec1)
        up2 = torch.cat([up2, enc3], dim=1)
        dec2 = self.decoder2(up2)
        dec2 = self.dropout(dec2)  # Aplicar dropout

        up3 = self.upconv3(dec2)
        up3 = torch.cat([up3, enc2], dim=1)
        dec3 = self.decoder3(up3)
        dec3 = self.dropout(dec3)  # Aplicar dropout

        up4 = self.upconv4(dec3)
        up4 = torch.cat([up4, enc1], dim=1)
        dec4 = self.decoder4(up4)
        dec4 = self.dropout(dec4)  # Aplicar dropout

        out = self.conv_out(dec4)
        out = self.sigmoid(out)
        out = self.norm(out)

        return out
    
#%%
# Uso de la función create_dataset
train_patches = train_patches.astype(np.float32)
train_humidity_patches = train_humidity_patches.astype(np.float32)



val_patches = val_patches.astype(np.float32)
val_humidity_patches = val_humidity_patches.astype(np.float32)


train_dataset, val_dataset = create_dataset(train_patches, val_patches, train_humidity_patches, val_humidity_patches)
#%%
import torch.nn as nn
import asyncio

#Aplicar la metrica de Error cuadratico medio
def trainer(train_dataset, val_dataset, model, args):
    # Configurar el dispositivo (GPU o CPU)
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # Establecer el dispositivo GPU a utilizar
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # Definir la función de pérdida y el optimizador
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])

    # Definir los dataloaders para el conjunto de datos de entrenamiento y validación
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args['val_batch_size'], shuffle=False)

    # Historial de entrenamiento
    history = {'train_loss': [], 'val_loss': []}
    val_accuracy_values = []  # Arreglo para almacenar los valores de precisión de validación

    # Entrenamiento del modelo
    for epoch in range(args['epochs']):
        # Modo de entrenamiento
        model.train()
        train_loss = 0.0
  #      val_accuracy = compute_accuracy(model, val_dataset)  # Calcular la precisión de validación
  #      val_accuracy_values.append(val_accuracy)  # Guardar el valor de precisión de validación en cada época

        for batch in train_loader:
            inputs = batch[0].to(device)
            targets = batch[1].to(device)

            # Reiniciar los gradientes
            optimizer.zero_grad()

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, targets)

            # Backward pass y optimización
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * inputs.size(0)

        # Calcular la pérdida promedio de entrenamiento
        train_loss /= len(train_dataset)
        history['train_loss'].append(train_loss)

        # Modo de evaluación
        model.eval()
        val_loss = 0.0

        with torch.no_grad():
            for batch in val_loader:
                inputs = batch[0].to(device)
                targets = batch[1].to(device)

                # Forward pass
                outputs = model(inputs)
                loss = criterion(outputs, targets)

                val_loss += loss.item() * inputs.size(0)

        # Calcular la pérdida promedio de validación
        val_loss /= len(val_dataset)
        history['val_loss'].append(val_loss)

        # Imprimir las métricas de entrenamiento y validación
        print(f'Epoch {epoch + 1}/{args["epochs"]}')
        print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')
        print('-' * 20)

    return history
#%%
import torch.nn as nn
#Aplicando el entrenamiendo a la métrica Error absoluto medio
def trainer_l1(train_dataset, val_dataset, model, args):
    # Configurar el dispositivo (GPU o CPU)
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # Establecer el dispositivo GPU a utilizar
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # Definir la función de pérdida y el optimizador
    criterion = nn.L1Loss()  # Error Absoluto Medio (MAE)
    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])

    # Definir los dataloaders para el conjunto de datos de entrenamiento y validación
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args['val_batch_size'], shuffle=False)

    # Historial de entrenamiento
    history = {'train_loss': [], 'val_loss': []}
    val_accuracy_values = []  # Arreglo para almacenar los valores de precisión de validación

    # Entrenamiento del modelo
    for epoch in range(args['epochs']):
        # Modo de entrenamiento
        model.train()
        train_loss = 0.0
 #       val_accuracy = compute_accuracy(model, val_dataset)  # Calcular la precisión de validación
 #       val_accuracy_values.append(val_accuracy)  # Guardar el valor de precisión de validación en cada época

        for batch in train_loader:
            inputs = batch[0].to(device)
            targets = batch[1].to(device)

            # Reiniciar los gradientes
            optimizer.zero_grad()

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, targets)

            # Backward pass y optimización
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * inputs.size(0)

        # Calcular la pérdida promedio de entrenamiento
        train_loss /= len(train_dataset)
        history['train_loss'].append(train_loss)

        # Modo de evaluación
        model.eval()
        val_loss = 0.0

        with torch.no_grad():
            for batch in val_loader:
                inputs = batch[0].to(device)
                targets = batch[1].to(device)

                # Forward pass
                outputs = model(inputs)
                loss = criterion(outputs, targets)

                val_loss += loss.item() * inputs.size(0)

        # Calcular la pérdida promedio de validación
        val_loss /= len(val_dataset)
        history['val_loss'].append(val_loss)

        # Imprimir las métricas de entrenamiento y validación
        print(f'Epoch {epoch + 1}/{args["epochs"]}')
        print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')
        print('-' * 20)

    return history
#%%
def trainer_entropy(train_dataset, val_dataset, model, args):
    # Configurar el dispositivo (GPU o CPU)
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # Establecer el dispositivo GPU a utilizar
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # Definir la función de pérdida y el optimizador
    criterion = nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])

    # Definir los dataloaders para el conjunto de datos de entrenamiento y validación
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args['val_batch_size'], shuffle=False)

    # Historial de entrenamiento
    history = {'train_loss': [], 'val_loss': []}

    # Entrenamiento del modelo
    for epoch in range(args['epochs']):
        # Modo de entrenamiento
        model.train()
        train_loss = 0.0

        for batch in train_loader:
            inputs = batch[0].to(device)
            targets = batch[1].to(device)

            # Reiniciar los gradientes
            optimizer.zero_grad()

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, targets)

            # Backward pass y optimización
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * inputs.size(0)

        # Calcular la pérdida promedio de entrenamiento
        train_loss /= len(train_dataset)
        history['train_loss'].append(train_loss)

        # Modo de evaluación
        model.eval()
        val_loss = 0.0

        with torch.no_grad():
            for batch in val_loader:
                inputs = batch[0].to(device)
                targets = batch[1].to(device)

                # Forward pass
                outputs = model(inputs)
                loss = criterion(outputs, targets)

                val_loss += loss.item() * inputs.size(0)

        # Calcular la pérdida promedio de validación
        val_loss /= len(val_dataset)
        history['val_loss'].append(val_loss)

        # Imprimir las métricas de entrenamiento y validación
        print(f'Epoch {epoch + 1}/{args["epochs"]}')
        print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')
        print('-' * 20)

    return history


#%%
# Intercambiar dimensiones de los tensores de entrenamiento de imágenes
train_patches = np.transpose(train_patches, (0, 3, 1, 2))
print(train_patches.shape)
# Intercambiar dimensiones de los tensores de validación de imágenes
val_patches = np.transpose(val_patches, (0, 3, 1, 2))

train_humidity_patches = np.transpose(train_humidity_patches, (0, 3, 1, 2))
# Intercambiar dimensiones de los tensores de prueba de imágenes
print(train_humidity_patches.shape)

val_humidity_patches = np.transpose(val_humidity_patches, (0, 3, 1, 2))
print(val_humidity_patches.shape)


#%%

#%%
import threading

# Definir los hiperparámetros
args = {
    'epochs': 2,
    'batch_size': 64,
    'val_batch_size': 16,
    'lr': 0.03
}

# Normalizar los datos Landsat
min_value = np.min(train_patches)
max_value = np.max(train_patches)

train_patches_normalized = (train_patches - min_value) / (max_value - min_value)

val_patches_normalized = (val_patches - min_value) / (max_value - min_value)

test_patches_normalized = (test_patches - min_value) / (max_value - min_value)


# Cambiar los valores negativos en los mapas de humedad del suelo por 0
train_humidity_patches[train_humidity_patches < 0] = 0

val_humidity_patches[val_humidity_patches < 0] = 0


# Convertir los arreglos a un tipo de dato compatible
train_patches_normalized = train_patches_normalized.astype(np.float32)
train_humidity_patches = train_humidity_patches.astype(np.float32)
val_patches_normalized = val_patches_normalized.astype(np.float32)
val_humidity_patches = val_humidity_patches.astype(np.float32)



# Crear instancias del modelo y el conjunto de datos
model1 = UNet(in_channels=8, out_channels=1)
model2 = UNet(in_channels=8, out_channels=1)
model3 = UNet(in_channels=8, out_channels=1)


train_dataset = PatchDataset(train_patches_normalized, train_humidity_patches)
val_dataset = PatchDataset(val_patches_normalized, val_humidity_patches)



#%%
#Implementacion del uso de la tecnica secuencial
import time

# Obtener el tiempo de inicio
start_time = time.time()

# Entrenar el modelo
start_time1 = time.time()
history = trainer(train_dataset, val_dataset, model1, args)
# Obtener el tiempo de finalización
end_time1 = time.time()
# Calcular la duración en segundos
duration11 = end_time1 - start_time1

start_time2 = time.time()
history2 = trainer_l1(train_dataset, val_dataset, model2, args)
# Obtener el tiempo de finalización
end_time2 = time.time()
# Calcular la duración en segundos
duration22 = end_time2 - start_time2

start_time3 = time.time()
history3 = trainer_entropy(train_dataset, val_dataset, model3, args)
end_time3 = time.time()
# Calcular la duración en segundos
duration33 = end_time3 - start_time3

# Obtener el tiempo de finalización
end_time = time.time()

# Calcular la duración en segundos
durationsecuencial = end_time - start_time
print("Tiempo de ejecución MSE:", duration11, "segundos")
print("Tiempo de ejecución MAE:", duration22, "segundos")
print("Tiempo de ejecución Entropy:", duration33, "segundos")

print("Tiempo de ejecución:", durationsecuencial, "segundos")


# Obtener los tiempos de ejecución de cada hilo
execution_times1 = [duration11, duration22, duration33]

# Definir la lista de números de hilos (pueden ser nombres de los modelos o índices)
model_names = ["MSE", "MAE", "ENTROPY"]

# Graficar el rendimiento del número de hilos por tiempo de ejecución
plt.bar(model_names, execution_times1)
plt.xlabel('Modelos')
plt.ylabel('Tiempo de Ejecución (segundos)')
plt.title('Tiempos de Ejecución por Modelo Técnica-Secuencial')
plt.grid(True)
# Agregar los tiempos de ejecución encima de cada barra
for i, v in enumerate(execution_times1):
    plt.text(i, v, f"{v:.2f}", ha='center', va='bottom')

plt.show()
#%%
#Implementando con numeros de recursos de CPU y haciendo uso de GPU
import time
import tensorflow as tf
import matplotlib.pyplot as plt

# Configuración de recursos
num_cpus = [1, 2, 4, 8]
use_gpu = True

# Listas para almacenar los tiempos de entrenamiento
tiempos = []

# Realizar experimentos para CPU
for num_cpu in num_cpus:
    # Configurar TensorFlow para usar el número de núcleos de CPU especificado
    tf.config.threading.set_inter_op_parallelism_threads(num_cpu)

    start_time = time.time()
    history = trainer(train_dataset, val_dataset, model1, args)
    history2 = trainer_l1(train_dataset, val_dataset, model2, args)
    history3 = trainer_entropy(train_dataset, val_dataset, model3, args)
    end_time = time.time()
    tiempo_entrenamiento = end_time - start_time
    tiempos.append(tiempo_entrenamiento)
print("Tiempo de ejecución por numeros de CPUs 1 2 4 8:", tiempos, "segundos")

# Realizar experimentos para GPU
if use_gpu:
    start_time = time.time()
    history_gpu = trainer(train_dataset, val_dataset, model1, args)
    history_gpu2 = trainer_l1(train_dataset, val_dataset, model2, args)
    history_gpu3 = trainer_entropy(train_dataset, val_dataset, model3, args)
    end_time = time.time()
    tiempo_entrenamiento_gpu = end_time - start_time
    tiempos.append(tiempo_entrenamiento_gpu)

# Crear un gráfico de barras comparativo
fig, ax = plt.subplots()
x_labels = [str(cpu) + ' CPU' for cpu in num_cpus]
x_labels.append('GPU')

bars = ax.bar(range(len(tiempos)), tiempos, align='center', tick_label=x_labels)
ax.set_xlabel('Recursos de Entrenamiento')
ax.set_ylabel('Tiempo de Entrenamiento por Época')
ax.set_title('Comparación de Tiempos de Entrenamiento: CPU vs. GPU')
plt.xticks(rotation=45)
plt.grid(True)

# Agregar los valores de tiempo de ejecución encima de cada barra
for bar, tiempo in zip(bars, tiempos):
    yval = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2.0, yval, round(tiempo, 2), ha='center', va='bottom', color='black', fontsize=9)

plt.show()
#%%
#Programacion MultiThreading para el modelo Se crearon tres hilos uno para cada valor de perdida o metrica y cada hilo ejecuto uno de ellos
import threading
import time
import matplotlib.pyplot as plt


# Función para entrenar el modelo 1
def train_model_1():
    global history1, duration1
    start_time1 = time.time()  # Guardar el tiempo de inicio
    history1 = trainer(train_dataset, val_dataset, model1, args)
    end_time1 = time.time()  # Guardar el tiempo de finalización
    duration1 = end_time1 - start_time1  # Guardar el tiempo de ejecución

# Función para entrenar el modelo 2
def train_model_2():
    global history2, duration2
    start_time2 = time.time()  # Guardar el tiempo de inicio
    history2 = trainer_l1(train_dataset, val_dataset, model2, args)
    end_time2 = time.time()  # Guardar el tiempo de finalización
    duration2 = end_time2 - start_time2  # Guardar el tiempo de ejecución

# Función para entrenar el modelo 3
def train_model_3():
    global history3, duration3
    start_time3 = time.time()  # Guardar el tiempo de inicio
    history3 = trainer_entropy(train_dataset, val_dataset, model3, args)
    end_time3 = time.time()  # Guardar el tiempo de finalización
    duration3 = end_time3 - start_time3  # Guardar el tiempo de ejecución

start_time = time.time()
# Crear los hilos para ejecutar las funciones de entrenamiento
thread1 = threading.Thread(target=train_model_1)
thread2 = threading.Thread(target=train_model_2)
thread3 = threading.Thread(target=train_model_3)

# Iniciar los hilos
thread1.start()
thread2.start()
thread3.start()

# Esperar a que los hilos terminen
thread1.join()
thread2.join()
thread3.join()

# Obtener el tiempo de finalización
end_time = time.time()

# Calcular la duración en segundos
durationmultithreading = end_time - start_time

print("Tiempo de ejecución:", durationmultithreading, "segundos")

# Obtener los tiempos de ejecución de cada hilo
execution_times = [duration1, duration2, duration3]

# Definir la lista de números de hilos (pueden ser nombres de los modelos o índices)
model_names = ["MSE", "MAE", "ENTROPY"]

# Graficar el rendimiento del número de hilos por tiempo de ejecución
plt.bar(model_names, execution_times)
plt.xlabel('Modelos')
plt.ylabel('Tiempo de Ejecución (segundos)')
plt.title('Tiempos de Ejecución por Modelo Técnica-Multithreading')
plt.grid(True)
# Agregar los tiempos de ejecución encima de cada barra
for i, v in enumerate(execution_times):
    plt.text(i, v, f"{v:.2f}", ha='center', va='bottom')

plt.show()
#%%
#Programacion MultiProcessing donde se tienen procesos y se evaluan los modelos en el mismo
import multiprocessing
import matplotlib.pyplot as plt

start_time = time.time()
# Listas para almacenar los resultados
num_procesos = []
duracion = []

if __name__ == '__main__':
    # Realizar el bucle de procesos
    for num_proc in range(1, 9):  # Prueba con diferentes números de procesos (1 a 8)
        # Crear los procesos para ejecutar las funciones
        procesos = []

        # Crear grupos de procesos para cada modelo
        grupo_procesos1 = []
        grupo_procesos2 = []
        grupo_procesos3 = []

        for _ in range(num_proc):
            modelo1 = multiprocessing.Process(target=trainer, args=(train_dataset, val_dataset, model1, args))
            grupo_procesos1.append(modelo1)

            modelo2 = multiprocessing.Process(target=trainer_l1, args=(train_dataset, val_dataset, model2, args))
            grupo_procesos2.append(modelo2)

            modelo3 = multiprocessing.Process(target=trainer_entropy, args=(train_dataset, val_dataset, model3, args))
            grupo_procesos3.append(modelo3)
        start_time1 = time.time()
        # Iniciar los procesos en cada grupo
        for modelo1 in grupo_procesos1:
            modelo1.start()
        for modelo2 in grupo_procesos2:
            modelo2.start()
        for modelo3 in grupo_procesos3:
            modelo3.start()

        # Esperar a que los procesos terminen en cada grupo
        for modelo1 in grupo_procesos1:
            modelo1.join()
        for modelo2 in grupo_procesos2:
            modelo2.join()
        for modelo3 in grupo_procesos3:
            modelo3.join()

        # Obtener el tiempo de finalización
        end_time1 = time.time()

        # Calcular la duración en segundos
        duration = end_time1 - start_time1

        # Agregar los resultados a las listas
        num_procesos.append(num_proc)
        duracion.append(duration)

        # Imprimir los tiempos de ejecución de cada grupo
        duracion_modelo1=time.time() - start_time1
        duracion_modelo2=time.time() - start_time1
        duracion_modelo3=time.time() - start_time1
        print(f"Número de procesos: {num_proc}")
        print("Tiempo de ejecución de MSE:", duracion_modelo1)
        print("Tiempo de ejecución de MAE:", duracion_modelo2)
        print("Tiempo de ejecución de Entropy:", duracion_modelo3)

    print("Multiprocessing completado")
# Obtener el tiempo de finalización
end_time = time.time()

# Calcular la duración en segundos
durationmultiprocessing = end_time - start_time



print("Tiempo de ejecución:", durationmultiprocessing, "segundos")

# Obtener los tiempos de ejecución de cada hilo
execution_times1 = [duracion_modelo1, duracion_modelo2, duracion_modelo3]

# Definir la lista de números de hilos (pueden ser nombres de los modelos o índices)
model_names1 = ["MSE", "MAE", "ENTROPY"]
# Graficar el rendimiento del número de hilos por tiempo de ejecución
plt.bar(model_names1, execution_times1)
plt.xlabel('Modelos')
plt.ylabel('Tiempo de Ejecución (segundos)')
plt.title('Tiempos de Ejecución por Modelo Técnica-Multiprocessing')
plt.grid(True)
# Agregar los tiempos de ejecución encima de cada barra
for i, v in enumerate(execution_times1):
    plt.text(i, v, f"{v:.2f}", ha='center', va='bottom')
plt.show()


# Graficar los tiempos de ejecución de los grupos
plt.plot(num_procesos, duracion, marker='o')
plt.xlabel('Número de Procesos')
plt.ylabel('Tiempo de Ejecución (segundos)')
plt.title('Tiempos de Ejecución por Número de Procesos')
plt.grid(True)
plt.show()
#%%

# Tecnica de Programacion concurrente
import concurrent.futures

# Definir una función wrapper para realizar el entrenamiento de un modelo
def train_model(train_data, val_data, model, args):
    # Obtener el tiempo de inicio
    start_time = time.time()
    # Realizar el entrenamiento del modelo y obtener el historial
    history = trainer(train_data, val_data, model, args)
    # Obtener el tiempo de finalización
    end_time = time.time()
    # Calcular la duración en segundos
    duration = end_time - start_time
    # Devolver el historial y la duración del modelo
    return history, duration

# Lista de modelos a entrenar (model1, model2, model3, etc.)
models = [model1, model2, model3]

# Crear una lista para almacenar los resultados de las tareas
results = []

# Obtener el tiempo de inicio
start_time = time.time()
# Crear un ThreadPoolExecutor con el número de hilos deseado
with concurrent.futures.ThreadPoolExecutor() as executor:
    # Ejecutar las tareas en paralelo
    futures = [executor.submit(train_model, train_dataset, val_dataset, model, args) for model in models]

    # Obtener los resultados de las tareas a medida que se completan
    for future in concurrent.futures.as_completed(futures):
        result = future.result()
        results.append(result)
# Obtener el tiempo de finalización
end_time = time.time()

# Calcular la duración en segundos del concurrent
durationconcurrent = end_time - start_time
# Asignar los resultados a los historiales correspondientes
history, duration1 = results[0]
history2, duration2 = results[1]
history3, duration3 = results[2]

print("Tiempo de ejecución Concurrent:", durationconcurrent, "segundos")
print("Tiempo de ejecución del MAE:", duration1, "segundos")
print("Tiempo de ejecución del MSE:", duration2, "segundos")
print("Tiempo de ejecución del Entropy:", duration3, "segundos")


# Obtener los tiempos de ejecución de cada hilo
execution_times2 = [duration1, duration2, duration3]

# Definir la lista de números de hilos (pueden ser nombres de los modelos o índices)
model_names1 = ["MSE", "MAE", "ENTROPY"]
# Graficar el rendimiento del número de hilos por tiempo de ejecución
plt.bar(model_names1, execution_times2)
plt.xlabel('Modelos')
plt.ylabel('Tiempo de Ejecución (segundos)')
plt.title('Tiempos de Ejecución por Modelo')
plt.grid(True)
# Agregar los tiempos de ejecución encima de cada barra
for i, v in enumerate(execution_times2):
    plt.text(i, v, f"{v:.2f}", ha='center', va='bottom')
plt.show()

#Resultados y comparacion de todas las tecnicas aplicadas
# Preparar los datos para la gráfica de barras
approaches = ['Secuencial', 'Multithreading', 'Multiprocessing', 'Concurrent']
execution_times = [durationsecuencial, durationmultithreading, durationconcurrent]

# Graficar los tiempos de ejecución
plt.bar(approaches, execution_times)
plt.xlabel('Enfoque de Entrenamiento')
plt.ylabel('Tiempo de Ejecución (segundos)')
plt.title('Comparación de Tiempos de Ejecución de las técnicas')
plt.grid(True)

# Agregar los valores de cada barra encima de ellas
for i, v in enumerate(execution_times):
    plt.text(i, v, f'{v:.2f}', ha='center', va='bottom')

plt.show()

#%%
#Cálculo del SpeedUP de MultiThreading y del MultiPreocessing
#El speedup representa cuánto más rápido se ejecuta el programa al aumentar la cantidad de recursos (hilos o procesos)
# Tiempo de ejecución secuencial
tiempo_secuencial = durationsecuencial

# Tiempo de ejecución con Multithreading
tiempo_multithreading = durationmultithreading

# Tiempo de ejecución con Multiprocessing
tiempo_multiprocessing = durationmultiprocessing

tiempo_concurrent = durationconcurrent

# Calcular el speedup
speedup_multithreading = tiempo_secuencial / tiempo_multithreading
speedup_multiprocessing = tiempo_secuencial / tiempo_multiprocessing
speedup_concurrent = tiempo_secuencial / tiempo_concurrent

print("El speedup representa cuánto más rápido se ejecuta el programa al aumentar la cantidad de recursos (hilos o procesos)")
print("Si el SpeedUP es mayor que 1 es señal de que hay mejora en el rendimiento")
print("Speedup Multithreading:", speedup_multithreading)
print("Speedup Multiprocessing:", speedup_multiprocessing)
print("Speedup Concurrent:", speedup_concurrent)

if speedup_multithreading > speedup_multiprocessing and speedup_multithreading > speedup_concurrent:
    print("El de mejor rendimiento es", speedup_multithreading)

elif  speedup_multiprocessing > speedup_multithreading and speedup_multiprocessing > speedup_concurrent:
    print("El de mejor rendimiento es", speedup_multiprocessing)

else:
  print("El de mejor rendimiento es", speedup_concurrent)

#%%
#Cálculo de la eficiencia de ambas tecnicas de MultiThreading y del MultiPreocessing
# Número de hilos o procesos utilizados
num_hilos = 3  # En el caso de Multithreading
num_procesos = num_proc  # En el caso de Multiprocessing
N=3 #N es el numero de hilos utilizados en el concurrent.futures.ThreadPoolExecutor

# Calcular la eficiencia para Multithreading
eficiencia_multithreading = speedup_multithreading / num_hilos

# Calcular la eficiencia para Multiprocessing
eficiencia_multiprocessing = speedup_multiprocessing / num_procesos

eficiencia_concurrent = speedup_concurrent / N

# Imprimir los resultados
print("Eficiencia Multithreading:", eficiencia_multithreading)
print("Eficiencia Multiprocessing:", eficiencia_multiprocessing)
print("Eficiencia Concurrent:", eficiencia_concurrent)

print("El de mejor eficiencia es el que se acerque más a 1.0:", eficiencia_multiprocessing)
#%%
#Cálculo de la Escalabilidad a ambas tecnicas de MultiThreading y del MultiPreocessing
import matplotlib.pyplot as plt

# Lista de tamaños de carga
tamanios_carga = [100, 200, 300, 400]  # Haciendo uso de tamaños de carga

# Listas para almacenar los resultados
escalabilidad_multithreading = []
escalabilidad_multiprocessing = []
escalabilidad_concurrent = []

for tamano_carga in tamanios_carga:
    # Ejecutar tu programa secuencialmente y medir el tiempo de ejecución secuencial
    tiempo_secuencial = durationsecuencial # Tiempo de ejecución secuencial para el tamaño de carga actual

    # Tiempo de ejecución con Multithreading
    tiempo_multithreading = durationmultithreading

    # Tiempo de ejecución con Multiprocessing
    tiempo_multiprocessing = durationmultiprocessing

        # Tiempo de ejecución con Multiprocessing
    tiempo_concurrent = durationconcurrent

    # Calcular el Speedup para el tamaño de carga actual
    speedup_multithreading = tiempo_secuencial / tiempo_multithreading
    speedup_multiprocessing = tiempo_secuencial / tiempo_multiprocessing
    speedup_concurrent = tiempo_secuencial / tiempo_concurrent

    # Calcular la escalabilidad para el tamaño de carga actual
    escalabilidad_multithreading.append(speedup_multithreading / tiempo_multithreading)
    escalabilidad_multiprocessing.append(speedup_multiprocessing / tiempo_multiprocessing)
    escalabilidad_concurrent.append(speedup_concurrent / tiempo_concurrent)


print("Es para evaluar cómo mejora el rendimiento de un sistema o algoritmo a medida que se incrementa la cantidad de recursos disponibles:")
# Imprimir los resultados
print("Escalabilidad Multithreading:", escalabilidad_multithreading)
print("Escalabilidad Multiprocessing:", escalabilidad_multiprocessing)
print("Escalabilidad Concurrent:", escalabilidad_concurrent)
#%%
#Cálculo de la Overhead a ambas tecnicas de MultiThreading y del MultiPreocessing
overheadTr = (durationsecuencial-durationmultithreading)/durationsecuencial

overheadPr = (durationsecuencial-durationmultiprocessing)/durationsecuencial

overheadCo = (durationsecuencial-durationconcurrent)/durationsecuencial
# Imprimir los resultados

print("Costo computacional o de recursos que implica la introducción de paralelismo o concurrencia en un programa:")
print("El Overhead del Multithreading:", overheadTr)
print("El Overhead del Multiprocessing:", overheadPr)
print("El Overhead del Concurrent:", overheadCo)
